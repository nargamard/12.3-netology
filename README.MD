# Домашнее задание к занятию "12.3 Развертывание кластера на собственных серверах, лекция 1"

### Задание 1: Описать требования к кластеру

``` 
Сначала проекту необходимо определить требуемые ресурсы. Известно, что 
проекту нужны база данных, система кеширования, а само приложение 
состоит из бекенда и фронтенда. Опишите, какие ресурсы нужны, если 
известно:

База данных должна быть отказоустойчивой. Потребляет 4 ГБ ОЗУ в 
работе, 1 ядро. 3 копии.
Кэш должен быть отказоустойчивый. Потребляет 4 ГБ ОЗУ в работе, 1 
ядро. 3 копии.
Фронтенд обрабатывает внешние запросы быстро, отдавая статику. 
Потребляет не более 50 МБ ОЗУ на каждый экземпляр, 0.2 ядра. 5 копий.
Бекенд потребляет 600 МБ ОЗУ и по 1 ядру на копию. 10 копий.
```

---
Ответ:

Составим таблицу потребных ресурсов исходя из задания:
| Сервис      | ОЗУ (Гб) | ЦП (ядер) | Реплика (шт) | ОЗУ всего (Гб) | ЦП всего (ядер) |
|-------------|----------|-----------|--------------|----------------|-----------------|
| База данных | 4        | 1         | 3            | 12             | 3               |
| Фронт       | 0,05     | 0,2       | 5            | 0,25           | 1               |
| Бэк         | 0,6      | 1         | 10           | 6              | 10              |
| Кэш         | 4        | 1         | 3            | 12             | 3               |
|             |          |           |              |                |                 |
| ИТОГО       |          |           |              | 30,25          | 17              |

Судя по потребным ресурсам, все поды можно вообще разместить на одной мощной ноде. Если не учитывать необходимость обеспечить безотказность работы кластера.

При этом можно разделить на разные ноды Рабочую и Управляющую:
| Функция ноды  | Количество нод | ОЗУ (Гб) | ЦП (ядер) | Диск (Гб) |   |
|---------------|----------------|----------|-----------|-----------|---|
| Control Plane | 1              | 4        | 4         | 128       |   |
| Worker Node   | 1              | 32       | 24        | 980       |   |

Представляется, что есть два крайних метода планирования числа нод:

1 - мало мощных нод, на которых запущено всё;

2 - много слабых нод, один под на ноду, один-два сервиса на под, в т.ч. вспомогательные вроде мониторинга.	

---

Представляется, что есть два крайних метода планирования числа нод:

1 - мало мощных нод, на которых запущено всё;

2 - много слабых нод, один под на ноду, один сервис на под.	

---

Если рассудить "за" и "против" для каждого случая, то получается, что мало мощных нод - хорошо в случае, если кластер разворачиваем на своём железе. Это даёт экономию, поскольку не требуется ставить несколько стоек с кучей слабых серверов. Поскольку и стойки, и корпусы, и писание, и лишние экземпляры более слабого оборудования (всё равно это сервер, а не домашний/офисный комп) - всё стоит денег. И обслуживание железе тоже стоит денег. Поэтому лучше, чтобы его было меньше. Но минус такой организации - потеря большой доли ресурсов при падении одной ноды. Оставшиеся могут не справиться.	

---

Большое число нод - дороже, сложнее обслуживать железо, но ничего критичного не произойдёт при выходе из стоя пары узлов. К тому же, если пренебречь знанием, что "нет никаких облаков - есть чужой компьютер", и арендовать ресурсы, то таки можно сделать много нод и дать им ровно столько ресурсов, сколько требуется каждому сервису и не больше.					
В этом случае расчет числа нод может выглядеть так:		
| Функция ноды        | Количество нод | ОЗУ (Гб) | ЦП (ядер) | Диск (Гб) |   |
|---------------------|----------------|----------|-----------|-----------|---|
| Control Plane       | 1              | 4        | 4         | 128       |   |
| Worker Node (БД)    | 3              | 4        | 1         | 980       |   |
| Worker Node (Фронт) | 5              | 1        | 1         | 128       |   |
| Worker Node (Бэк)   | 10             | 1        | 1         | 128       |   |
| Worker Node (Кэш)   | 3              | 4        | 1         | 128       |   |
|                     |                |          |           |           |   |			


С учетом необходимости обеспечить отказоустойчивость, число нод для Фронта и Бэка следует увеличить (для БД и кэша отказоустойчивость уже обеспечена), но даже и без этого их 22 штуки, что много.

Таким образом, истина где-то посередине. Если предельное число копий у Бэка (10 нод), то этим числом и ограничимся:
| Номер ноды | Функция ноды     | ОЗУ (Гб) | ЦП (ядер) | Диск (Гб) |
|------------|------------------|----------|-----------|-----------|
| 1          | Control, БД, бэк | 9        | 6         | 980       |
| 2          | Control, БД, бэк | 9        | 6         | 980       |
| 3          | Control, БД, бэк | 9        | 6         | 980       |
| 4          | Фронт, бэк, кэш  | 5        | 3         | 128       |
| 5          | Фронт, бэк       | 1        | 2         | 128       |
| 6          | Фронт, бэк       | 1        | 2         | 128       |
| 7          | Фронт, бэк       | 1        | 2         | 128       |
| 8          | Фронт, бэк       | 1        | 2         | 128       |
| 9          | Кэш, бэк         | 5        | 2         | 128       |
| 10         | Кэш, бэк         | 5        | 2         | 128       |

Здесь значения ОЗУ и ЦП минимально достаточные. Теперь увеличим эти значения с учетом возможного выхода из строя одной ноды. Самые ресурсоёмкие ноды - №1,2,3. Добавим эти ресурсы четвёртой, а ресурсы четвёртой - к пятой. Таким образом, все поды с упавшей ноды смогут запуститься на одной резервной ноде.					
| Номер ноды | Функция ноды     | ОЗУ (Гб) | ЦП (ядер) | Диск (Гб) |
|------------|------------------|----------|-----------|-----------|
| 1          | Control, БД, бэк | 9        | 6         | 980       |
| 2          | Control, БД, бэк | 9        | 6         | 980       |
| 3          | Control, БД, бэк | 9        | 6         | 980       |
| 4          | Фронт, бэк, кэш  | 14!      | 9!        | 128       |
| 5          | Фронт, бэк       | 6!       | 5!        | 128       |
| 6          | Фронт, бэк       | 1        | 2         | 128       |
| 7          | Фронт, бэк       | 1        | 2         | 128       |
| 8          | Фронт, бэк       | 1        | 2         | 128       |
| 9          | Кэш, бэк         | 5        | 2         | 128       |
| 10         | Кэш, бэк         | 5        | 2         | 128       |

Теперь добавим сюда ресурсы для системы мониторинга и сбора логов, сущностей, которые надо мониторить, здесь немного, поэтому для Prometheus будем считать достаточным 4Гб ОЗУ и 2 ядра.

А для сбора логов 8Гб ОЗУ и 4 ядра, поскольку не известно, насколько много логов будет генерироваться (зависит от приложения). 

Разместим на 6,7 ноде Prometheus, а на 9,10 нодах компонены ELK. 

Тогда для возможности запуска подов с упавшей ноды лучше изменить ресурсы не четвертой ноды, а восьмой - самой слабой. А ресурсы восьмой прибавим к пятой.					
| Номер ноды | Функция ноды           | ОЗУ (Гб) | ЦП (ядер) | Диск (Гб) |
|------------|------------------------|----------|-----------|-----------|
| 1          | Control, БД, бэк       | 9        | 6         | 980       |
| 2          | Control, БД, бэк       | 9        | 6         | 980       |
| 3          | Control, БД, бэк       | 9        | 6         | 980       |
| 4          | Фронт, бэк, кэш        | 5!       | 3!        | 128       |
| 5          | Фронт, бэк             | 2!       | 4!        | 128       |
| 6          | Фронт, бэк, Prometheus | 5!       | 4!        | 128       |
| 7          | Фронт, бэк, Prometheus | 5!       | 4!        | 128       |
| 8          | Фронт, бэк             | 14!      | 8!        | 128       |
| 9          | Кэш, бэк, ELK          | 13!      | 6!        | 480       |
| 10         | Кэш, бэк, ELK          | 13!      | 6!        | 480       |

**Будем считать эту таблицу итоговой в том случае, если кластер будет работать в облаке.**

 Если же разворачиваем на своём железе, тогда я бы сделал так:
 | Номер ноды | Функция ноды           | ОЗУ (Гб) | ЦП (ядер) | Диск (Гб) |
|------------|------------------------|----------|-----------|-----------|
| 1          | Control, БД, бэк       | 16       | 8         | 980       |
| 2          | Control, БД, бэк       | 16       | 8         | 980       |
| 3          | Control, БД, бэк       | 16       | 8         | 980       |
| 4          | Фронт, бэк, кэш        | 16       | 8         | 980       |
| 5          | Фронт, бэк             | 16       | 8         | 980       |
| 6          | Фронт, бэк, Prometheus | 16       | 8         | 980       |
| 7          | Фронт, бэк, Prometheus | 16       | 8         | 980       |
| 8          | Фронт, бэк             | 16       | 8         | 980       |
| 9          | Кэш, бэк, ELK          | 16       | 8         | 980       |
| 10         | Кэш, бэк, ELK          | 16       | 8         | 980       |

10 одинаковых нод с ресурсами, значительно перекрывающими потребности в случае выхода из строя более чем одной ноды и обеспечивающими кроме масштабирование сервисов.

Кроме того, легко можно сделать все по 32 ОЗУ, либо добавлять позднее (зависит он железа) и ЦПУ сразу поставить на 12 ядер. Но это уже зависит от планов по будущему масштабированию.